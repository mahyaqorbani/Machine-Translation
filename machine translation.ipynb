{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the essential libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    \n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "filename = 'first dataset.txt'\n",
    "doc = load_doc(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a loaded document into sentences\n",
    "def to_pairs(doc):\n",
    "    #to seprate lines\n",
    "    lines = doc.strip().split('\\n')\n",
    "    #to seprate english and german phrases in each line\n",
    "    pairs = [line.split('\\t') for line in  lines]\n",
    "    return pairs\n",
    "\n",
    "# split into english-german pairs\n",
    "pairs = to_pairs(doc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hi.', 'Hallo!'],\n",
       " ['Hi.', 'Grüß Gott!'],\n",
       " ['Run!', 'Lauf!'],\n",
       " ['Wow!', 'Potzdonner!'],\n",
       " ['Wow!', 'Donnerwetter!']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean a list of lines\n",
    "def clean_pairs(lines):\n",
    "    cleaned = list()\n",
    "    # prepare regex for char filtering: we use it as a func. to remove unprintable chars in each phrase\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table for removing punctuation,\n",
    "    #the keys will be ASCII code of every punctuations and the values are None: it will work as a mapper\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for pair in lines:\n",
    "        clean_pair = list()\n",
    "        for line in pair:\n",
    "            # tokenize on white space\n",
    "            line = line.split()\n",
    "            # convert to lowercase\n",
    "            line = [word.lower() for word in line]\n",
    "            # remove punctuation from each token(word): map each punctuation to ''\n",
    "            line = [word.translate(table) for word in line]\n",
    "            # remove non-printable chars form each token(word)\n",
    "            line = [re_print.sub('', w) for w in line]\n",
    "            # remove tokens with numbers in them\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            # store as string : put back the words to a sentnce\n",
    "            clean_pair.append(' '.join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "    return array(cleaned)\n",
    "\n",
    "# clean sentences\n",
    "clean_pairs = clean_pairs(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['am i fat', 'bin ich dick'], dtype='<U367')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_pairs[177]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Am I fat?', 'Bin ich dick?']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[177]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-german.pkl\n"
     ]
    }
   ],
   "source": [
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)\n",
    "\n",
    "# save clean pairs to file\n",
    "save_clean_data(clean_pairs, 'english-german.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hallo] => [hi]\n",
      "[gr gott] => [hi]\n",
      "[lauf] => [run]\n",
      "[potzdonner] => [wow]\n",
      "[donnerwetter] => [wow]\n",
      "[feuer] => [fire]\n",
      "[hilfe] => [help]\n",
      "[zu hlf] => [help]\n",
      "[stopp] => [stop]\n",
      "[warte] => [wait]\n"
     ]
    }
   ],
   "source": [
    "# To check\n",
    "for i in range(10):\n",
    "    print('[%s] => [%s]' % (clean_pairs[i,1], clean_pairs[i,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152820, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_pairs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-german-both.pkl\n",
      "Saved: english-german-train.pkl\n",
      "Saved: english-german-test.pkl\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from pickle import dump\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle\n",
    "  \n",
    "# create dataset\n",
    "raw_dataset = clean_pairs\n",
    " \n",
    "# reduce dataset size\n",
    "n_sentences = 20000\n",
    "dataset = raw_dataset[:n_sentences, :]\n",
    "# random shuffle\n",
    "shuffle(dataset)\n",
    "# split into train/test\n",
    "train, test = dataset[:18500], dataset[18500:]\n",
    "\n",
    "# save\n",
    "save_clean_data(dataset, 'english-german-both.pkl')\n",
    "save_clean_data(train, 'english-german-train.pkl')\n",
    "save_clean_data(test, 'english-german-test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Prepare inputs of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    " \n",
    "# find the maximum length of phrases (in both germany and english)\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)\n",
    " \n",
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    #integer encode sequences: map words to integers\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    #pad sequences with 0 values: make all the sequences' length the same by adding 0 at the end of the shorter ones\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X\n",
    " \n",
    "# one hot encode target sequence\n",
    "#This is because the model will predict the probability of each word in the vocabulary as output.\n",
    "def encode_output(sequences, vocab_size):\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        #each word will be one hot encoded and convert to a vector of eng/ger vocab size: \n",
    "        #for each phrase we will have (eng/ger length * eng/ger vocab size) vector\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = array(ylist)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 3816\n",
      "English Max Length: 6\n",
      "German Vocabulary Size: 6240\n",
      "German Max Length: 10\n"
     ]
    }
   ],
   "source": [
    "# prepare english tokenizer\n",
    "#it will map each word to an integer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "\n",
    "#count the unique integers to find out the vocabulary size\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "\n",
    "#find the maximum length of phrases in English\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))\n",
    "\n",
    "# prepare german tokenizer : do all the above for German phrases\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "\n",
    "print('German Vocabulary Size: %d' % ger_vocab_size)\n",
    "print('German Max Length: %d' % (ger_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "trainY = encode_output(trainY, eng_vocab_size)\n",
    "# prepare test data\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "testY = encode_output(testY, eng_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 10,  30,   5, 575, 156,   0,   0,   0,   0,   0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 10, 512)           3194880   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 6, 512)            0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 6, 512)            2099200   \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 6, 3816)           1957608   \n",
      "=================================================================\n",
      "Total params: 9,350,888\n",
      "Trainable params: 9,350,888\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint\n",
    " \n",
    "\n",
    "# define NMT model\n",
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(n_units))\n",
    "    model.add(RepeatVector(tar_timesteps))\n",
    "    model.add(LSTM(n_units, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "    return model\n",
    "\n",
    "# define model\n",
    "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 512)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "# summarize defined model\n",
    "print(model.summary())\n",
    "#you should run in colab to get the picture of model structure\n",
    "plot_model(model, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 10, 512)           3194880   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 6, 512)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 6, 512)            2099200   \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 6, 3816)           1957608   \n",
      "=================================================================\n",
      "Total params: 9,350,888\n",
      "Trainable params: 9,350,888\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n",
      "Epoch 1/35\n",
      "290/290 [==============================] - 112s 362ms/step - loss: 4.4627 - val_loss: 3.4309\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.43089, saving model to model.h5\n",
      "Epoch 2/35\n",
      "290/290 [==============================] - 100s 344ms/step - loss: 3.2606 - val_loss: 3.1354\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.43089 to 3.13537, saving model to model.h5\n",
      "Epoch 3/35\n",
      "290/290 [==============================] - 102s 352ms/step - loss: 2.8596 - val_loss: 2.7749\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.13537 to 2.77495, saving model to model.h5\n",
      "Epoch 4/35\n",
      "290/290 [==============================] - 103s 354ms/step - loss: 2.4538 - val_loss: 2.5074\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.77495 to 2.50736, saving model to model.h5\n",
      "Epoch 5/35\n",
      "290/290 [==============================] - 101s 350ms/step - loss: 2.1033 - val_loss: 2.2937\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.50736 to 2.29373, saving model to model.h5\n",
      "Epoch 6/35\n",
      "290/290 [==============================] - 100s 345ms/step - loss: 1.7884 - val_loss: 2.1124\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.29373 to 2.11237, saving model to model.h5\n",
      "Epoch 7/35\n",
      "290/290 [==============================] - 100s 346ms/step - loss: 1.5203 - val_loss: 1.9801\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.11237 to 1.98011, saving model to model.h5\n",
      "Epoch 8/35\n",
      "290/290 [==============================] - 100s 345ms/step - loss: 1.2795 - val_loss: 1.8759\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.98011 to 1.87592, saving model to model.h5\n",
      "Epoch 9/35\n",
      "290/290 [==============================] - 99s 341ms/step - loss: 1.0766 - val_loss: 1.8046\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.87592 to 1.80462, saving model to model.h5\n",
      "Epoch 10/35\n",
      "290/290 [==============================] - 101s 347ms/step - loss: 0.8996 - val_loss: 1.7461\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.80462 to 1.74611, saving model to model.h5\n",
      "Epoch 11/35\n",
      "290/290 [==============================] - 99s 342ms/step - loss: 0.7417 - val_loss: 1.7039\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.74611 to 1.70392, saving model to model.h5\n",
      "Epoch 12/35\n",
      "290/290 [==============================] - 99s 341ms/step - loss: 0.6206 - val_loss: 1.6710\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.70392 to 1.67104, saving model to model.h5\n",
      "Epoch 13/35\n",
      "290/290 [==============================] - 98s 339ms/step - loss: 0.5126 - val_loss: 1.6533\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.67104 to 1.65328, saving model to model.h5\n",
      "Epoch 14/35\n",
      "290/290 [==============================] - 99s 340ms/step - loss: 0.4236 - val_loss: 1.6478\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.65328 to 1.64780, saving model to model.h5\n",
      "Epoch 15/35\n",
      "290/290 [==============================] - 99s 343ms/step - loss: 0.3604 - val_loss: 1.6291\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.64780 to 1.62909, saving model to model.h5\n",
      "Epoch 16/35\n",
      "290/290 [==============================] - 99s 342ms/step - loss: 0.2931 - val_loss: 1.6636\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.62909\n",
      "Epoch 17/35\n",
      "290/290 [==============================] - 101s 348ms/step - loss: 0.2509 - val_loss: 1.6406\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.62909\n",
      "Epoch 18/35\n",
      "290/290 [==============================] - 100s 344ms/step - loss: 0.2227 - val_loss: 1.6446\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.62909\n",
      "Epoch 19/35\n",
      "290/290 [==============================] - 99s 343ms/step - loss: 0.1991 - val_loss: 1.6702\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.62909\n",
      "Epoch 20/35\n",
      "290/290 [==============================] - 101s 347ms/step - loss: 0.1729 - val_loss: 1.6871\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.62909\n",
      "Epoch 21/35\n",
      "290/290 [==============================] - 102s 352ms/step - loss: 0.1550 - val_loss: 1.7022\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.62909\n",
      "Epoch 22/35\n",
      "290/290 [==============================] - 99s 343ms/step - loss: 0.1383 - val_loss: 1.7109\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.62909\n",
      "Epoch 23/35\n",
      "290/290 [==============================] - 98s 339ms/step - loss: 0.1311 - val_loss: 1.7205\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.62909\n",
      "Epoch 24/35\n",
      "290/290 [==============================] - 99s 343ms/step - loss: 0.1262 - val_loss: 1.7371\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.62909\n",
      "Epoch 25/35\n",
      "290/290 [==============================] - 101s 348ms/step - loss: 0.1239 - val_loss: 1.7679\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.62909\n",
      "Epoch 26/35\n",
      "290/290 [==============================] - 95s 328ms/step - loss: 0.1210 - val_loss: 1.7668\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.62909\n",
      "Epoch 27/35\n",
      "290/290 [==============================] - 95s 327ms/step - loss: 0.1100 - val_loss: 1.7642\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.62909\n",
      "Epoch 28/35\n",
      "290/290 [==============================] - 100s 347ms/step - loss: 0.1144 - val_loss: 1.7891\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.62909\n",
      "Epoch 29/35\n",
      "290/290 [==============================] - 101s 349ms/step - loss: 0.1090 - val_loss: 1.7968\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.62909\n",
      "Epoch 30/35\n",
      "290/290 [==============================] - 101s 349ms/step - loss: 0.1050 - val_loss: 1.8052\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.62909\n",
      "Epoch 31/35\n",
      "290/290 [==============================] - 99s 341ms/step - loss: 0.1060 - val_loss: 1.8138\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.62909\n",
      "Epoch 32/35\n",
      "290/290 [==============================] - 100s 346ms/step - loss: 0.1057 - val_loss: 1.8090\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.62909\n",
      "Epoch 33/35\n",
      "290/290 [==============================] - 100s 343ms/step - loss: 0.0973 - val_loss: 1.8272\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.62909\n",
      "Epoch 34/35\n",
      "290/290 [==============================] - 101s 348ms/step - loss: 0.0980 - val_loss: 1.8569\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.62909\n",
      "Epoch 35/35\n",
      "290/290 [==============================] - 103s 355ms/step - loss: 0.1082 - val_loss: 1.8871\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.62909\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x23f5ed9d8e0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint\n",
    " \n",
    "\n",
    "# define NMT model\n",
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(n_units))\n",
    "    model.add(RepeatVector(tar_timesteps))\n",
    "    model.add(LSTM(n_units, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "    return model\n",
    "\n",
    "# define model\n",
    "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 512)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "# summarize defined model\n",
    "print(model.summary())\n",
    "plot_model(model, to_file='model.png', show_shapes=True)\n",
    "# fit model\n",
    "filename = 'model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(trainX, trainY, epochs=35, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "src=[wir haben das spiel verloren], target=[we lost the game], predicted=[we lost the game]\n",
      "src=[ich bin schtig], target=[im addicted], predicted=[im addicted]\n",
      "src=[er schien ehrlich zu sein], target=[he appeared honest], predicted=[he appeared honest]\n",
      "src=[tom ist starrkpfig], target=[tom is stubborn], predicted=[tom is stubborn]\n",
      "src=[ich bin lehrerin], target=[i am a teacher], predicted=[im am teacher teacher]\n",
      "src=[ich halte das nicht aus], target=[i cant stand it], predicted=[i cant stand that]\n",
      "src=[wo ist der rest], target=[wheres the rest], predicted=[wheres the rest]\n",
      "src=[tom wird schreien], target=[tom will scream], predicted=[tom will scream]\n",
      "src=[niemand glaubt mir], target=[no one believes me], predicted=[nobody one believes me]\n",
      "src=[tom wurde gewaltttig], target=[tom became violent], predicted=[tom became violent]\n",
      "BLEU-1: 0.903378\n",
      "BLEU-2: 0.861825\n",
      "BLEU-3: 0.818883\n",
      "BLEU-4: 0.658737\n",
      "test\n",
      "src=[entspannen sie sich bitte], target=[please relax], predicted=[please have]\n",
      "src=[tom ist nicht unschuldig], target=[tom isnt innocent], predicted=[tom isnt]\n",
      "src=[wie hat tom reagiert], target=[how did tom react], predicted=[how did tom respond]\n",
      "src=[du warst beschftigt], target=[you were busy], predicted=[you were busy]\n",
      "src=[tom ist geschwommen], target=[tom swam], predicted=[tom swam]\n",
      "src=[komm wieder], target=[come back], predicted=[come again]\n",
      "src=[tom hat einen plan], target=[tom has a plan], predicted=[tom has a ring]\n",
      "src=[hier regnet es], target=[its raining here], predicted=[its it here here]\n",
      "src=[ich komm im moment nicht auf seinen namen], target=[i forget his name], predicted=[im on my]\n",
      "src=[ist er schon da], target=[has he arrived yet], predicted=[is he arrived yet]\n",
      "BLEU-1: 0.594743\n",
      "BLEU-2: 0.471391\n",
      "BLEU-3: 0.407985\n",
      "BLEU-4: 0.265060\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    " \n",
    "# generate target given source sequence\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "    prediction = model.predict(source, verbose=0)[0]\n",
    "    integers = [argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = word_for_id(i, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return ' '.join(target)\n",
    " \n",
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    "    actual, predicted = list(), list()\n",
    "    for i, source in enumerate(sources):\n",
    "        # translate encoded source text\n",
    "        source = source.reshape((1, source.shape[0]))\n",
    "        translation = predict_sequence(model, eng_tokenizer, source)\n",
    "        raw_target, raw_src = raw_dataset[i,:2]\n",
    "        if i < 10:\n",
    "            print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
    "        actual.append([raw_target.split()])\n",
    "        predicted.append(translation.split())\n",
    "    # calculate BLEU score\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "\n",
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "# prepare german tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "# prepare data\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    " \n",
    "# load model\n",
    "model = load_model('model.h5')\n",
    "# test on some training sequences\n",
    "print('train')\n",
    "evaluate_model(model, eng_tokenizer, trainX, train)\n",
    "# test on some test sequences\n",
    "print('test')\n",
    "evaluate_model(model, eng_tokenizer, testX, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n_unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 10, 256)           1597440   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 6, 256)            0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 6, 256)            525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 6, 3816)           980712    \n",
      "=================================================================\n",
      "Total params: 3,628,776\n",
      "Trainable params: 3,628,776\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n",
      "Epoch 1/35\n",
      "290/290 [==============================] - 75s 205ms/step - loss: 4.8600 - val_loss: 3.4971\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.49706, saving model to model.h5\n",
      "Epoch 2/35\n",
      "290/290 [==============================] - 53s 183ms/step - loss: 3.3870 - val_loss: 3.3578\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.49706 to 3.35782, saving model to model.h5\n",
      "Epoch 3/35\n",
      "290/290 [==============================] - 53s 184ms/step - loss: 3.2230 - val_loss: 3.2109\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.35782 to 3.21090, saving model to model.h5\n",
      "Epoch 4/35\n",
      "290/290 [==============================] - 53s 183ms/step - loss: 3.0434 - val_loss: 3.0374\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.21090 to 3.03741, saving model to model.h5\n",
      "Epoch 5/35\n",
      "290/290 [==============================] - 53s 183ms/step - loss: 2.8014 - val_loss: 2.7920\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.03741 to 2.79196, saving model to model.h5\n",
      "Epoch 6/35\n",
      "290/290 [==============================] - 54s 188ms/step - loss: 2.5354 - val_loss: 2.6094\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.79196 to 2.60944, saving model to model.h5\n",
      "Epoch 7/35\n",
      "290/290 [==============================] - 54s 184ms/step - loss: 2.3057 - val_loss: 2.4581\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.60944 to 2.45811, saving model to model.h5\n",
      "Epoch 8/35\n",
      "290/290 [==============================] - 53s 183ms/step - loss: 2.1138 - val_loss: 2.3333\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.45811 to 2.33328, saving model to model.h5\n",
      "Epoch 9/35\n",
      "290/290 [==============================] - 53s 182ms/step - loss: 1.9351 - val_loss: 2.2293\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.33328 to 2.22929, saving model to model.h5\n",
      "Epoch 10/35\n",
      "290/290 [==============================] - 53s 182ms/step - loss: 1.7692 - val_loss: 2.1454\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.22929 to 2.14543, saving model to model.h5\n",
      "Epoch 11/35\n",
      "290/290 [==============================] - 53s 183ms/step - loss: 1.6321 - val_loss: 2.0772\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.14543 to 2.07718, saving model to model.h5\n",
      "Epoch 12/35\n",
      "290/290 [==============================] - 54s 186ms/step - loss: 1.5055 - val_loss: 2.0246\n",
      "\n",
      "Epoch 00012: val_loss improved from 2.07718 to 2.02462, saving model to model.h5\n",
      "Epoch 13/35\n",
      "290/290 [==============================] - 54s 185ms/step - loss: 1.3657 - val_loss: 1.9472\n",
      "\n",
      "Epoch 00013: val_loss improved from 2.02462 to 1.94725, saving model to model.h5\n",
      "Epoch 14/35\n",
      "290/290 [==============================] - 54s 185ms/step - loss: 1.2436 - val_loss: 1.8971\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.94725 to 1.89707, saving model to model.h5\n",
      "Epoch 15/35\n",
      "290/290 [==============================] - 53s 183ms/step - loss: 1.1240 - val_loss: 1.8614\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.89707 to 1.86139, saving model to model.h5\n",
      "Epoch 16/35\n",
      "290/290 [==============================] - 54s 185ms/step - loss: 1.0172 - val_loss: 1.8318\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.86139 to 1.83181, saving model to model.h5\n",
      "Epoch 17/35\n",
      "290/290 [==============================] - 55s 188ms/step - loss: 0.9207 - val_loss: 1.7919\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.83181 to 1.79190, saving model to model.h5\n",
      "Epoch 18/35\n",
      "290/290 [==============================] - 53s 184ms/step - loss: 0.8264 - val_loss: 1.7605\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.79190 to 1.76048, saving model to model.h5\n",
      "Epoch 19/35\n",
      "290/290 [==============================] - 53s 184ms/step - loss: 0.7514 - val_loss: 1.7532\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.76048 to 1.75317, saving model to model.h5\n",
      "Epoch 20/35\n",
      "290/290 [==============================] - 53s 184ms/step - loss: 0.6794 - val_loss: 1.7402\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.75317 to 1.74016, saving model to model.h5\n",
      "Epoch 21/35\n",
      "290/290 [==============================] - 53s 183ms/step - loss: 0.6084 - val_loss: 1.7250\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.74016 to 1.72497, saving model to model.h5\n",
      "Epoch 22/35\n",
      "290/290 [==============================] - 53s 184ms/step - loss: 0.5591 - val_loss: 1.7164\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.72497 to 1.71643, saving model to model.h5\n",
      "Epoch 23/35\n",
      "290/290 [==============================] - 53s 184ms/step - loss: 0.5036 - val_loss: 1.7109\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.71643 to 1.71089, saving model to model.h5\n",
      "Epoch 24/35\n",
      "290/290 [==============================] - 53s 183ms/step - loss: 0.4520 - val_loss: 1.7109\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.71089 to 1.71087, saving model to model.h5\n",
      "Epoch 25/35\n",
      "290/290 [==============================] - 53s 182ms/step - loss: 0.4117 - val_loss: 1.7102\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.71087 to 1.71016, saving model to model.h5\n",
      "Epoch 26/35\n",
      "290/290 [==============================] - 53s 183ms/step - loss: 0.3585 - val_loss: 1.7001\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.71016 to 1.70007, saving model to model.h5\n",
      "Epoch 27/35\n",
      "290/290 [==============================] - 53s 182ms/step - loss: 0.3407 - val_loss: 1.7189\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.70007\n",
      "Epoch 28/35\n",
      "290/290 [==============================] - 53s 183ms/step - loss: 0.3093 - val_loss: 1.7136\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.70007\n",
      "Epoch 29/35\n",
      "290/290 [==============================] - 53s 183ms/step - loss: 0.2792 - val_loss: 1.7204\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.70007\n",
      "Epoch 30/35\n",
      "290/290 [==============================] - 53s 184ms/step - loss: 0.2570 - val_loss: 1.7218\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.70007\n",
      "Epoch 31/35\n",
      "290/290 [==============================] - 53s 183ms/step - loss: 0.2371 - val_loss: 1.7231\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.70007\n",
      "Epoch 32/35\n",
      "290/290 [==============================] - 53s 183ms/step - loss: 0.2127 - val_loss: 1.7399\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.70007\n",
      "Epoch 33/35\n",
      "290/290 [==============================] - 53s 184ms/step - loss: 0.1933 - val_loss: 1.7373\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.70007\n",
      "Epoch 34/35\n",
      "290/290 [==============================] - 54s 185ms/step - loss: 0.1927 - val_loss: 1.7532\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.70007\n",
      "Epoch 35/35\n",
      "290/290 [==============================] - 54s 186ms/step - loss: 0.1794 - val_loss: 1.7702\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.70007\n",
      "train\n",
      "src=[vermissen sie boston], target=[do you miss boston], predicted=[do you miss boston]\n",
      "src=[kann ich ihn nach hause mitnehmen], target=[can i take it home], predicted=[can i take it home]\n",
      "src=[fhlst du dich krank], target=[do you feel sick], predicted=[do you feel sick]\n",
      "src=[sie spricht schnell], target=[she talks quickly], predicted=[she speaks quickly]\n",
      "src=[ich werde mit ihm schimpfen], target=[ill scold him], predicted=[ill scold him]\n",
      "src=[sie ist nicht arm], target=[she isnt poor], predicted=[she not poor]\n",
      "src=[lass uns jetzt gehen], target=[lets go now], predicted=[lets go now]\n",
      "src=[was ist so lustig], target=[whats so funny], predicted=[what so funny]\n",
      "src=[tom war eiferschtig], target=[tom was jealous], predicted=[tom was jealous]\n",
      "src=[mach das licht aus], target=[turn off the light], predicted=[turn off the light]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.891970\n",
      "BLEU-2: 0.843942\n",
      "BLEU-3: 0.798830\n",
      "BLEU-4: 0.635490\n",
      "test\n",
      "src=[das ist meine tasche], target=[this is my bag], predicted=[this bag my bag]\n",
      "src=[hast du kinder], target=[do you have kids], predicted=[do you have kids]\n",
      "src=[seid ihr noch da], target=[are you still there], predicted=[are you still there]\n",
      "src=[da sind wir], target=[here we are], predicted=[here we are]\n",
      "src=[tom ist innovativ], target=[tom is innovative], predicted=[tom is devious]\n",
      "src=[sie ist aufgeschlossen], target=[shes openminded], predicted=[she is]\n",
      "src=[ich versuche es noch mal], target=[ill try again], predicted=[i try it it]\n",
      "src=[wo ist meine uhr], target=[where is my watch], predicted=[wheres is my clock]\n",
      "src=[ich brauche einen job], target=[i need a job], predicted=[i need a job]\n",
      "src=[das wusste ich schon], target=[i already knew that], predicted=[i already knew that]\n",
      "BLEU-1: 0.587024\n",
      "BLEU-2: 0.468558\n",
      "BLEU-3: 0.410816\n",
      "BLEU-4: 0.266693\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 10, 128)           798720    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 6, 128)            0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 6, 128)            131584    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 6, 3816)           492264    \n",
      "=================================================================\n",
      "Total params: 1,554,152\n",
      "Trainable params: 1,554,152\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n",
      "Epoch 1/35\n",
      "290/290 [==============================] - 38s 113ms/step - loss: 5.2768 - val_loss: 3.7937\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.79367, saving model to model.h5\n",
      "Epoch 2/35\n",
      "290/290 [==============================] - 34s 117ms/step - loss: 3.6828 - val_loss: 3.6011\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.79367 to 3.60109, saving model to model.h5\n",
      "Epoch 3/35\n",
      "290/290 [==============================] - 32s 109ms/step - loss: 3.4862 - val_loss: 3.4618\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.60109 to 3.46184, saving model to model.h5\n",
      "Epoch 4/35\n",
      "290/290 [==============================] - 32s 109ms/step - loss: 3.3635 - val_loss: 3.3909\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.46184 to 3.39091, saving model to model.h5\n",
      "Epoch 5/35\n",
      "290/290 [==============================] - 32s 109ms/step - loss: 3.2633 - val_loss: 3.2893\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.39091 to 3.28932, saving model to model.h5\n",
      "Epoch 6/35\n",
      "290/290 [==============================] - 31s 108ms/step - loss: 3.1439 - val_loss: 3.1732\n",
      "\n",
      "Epoch 00006: val_loss improved from 3.28932 to 3.17320, saving model to model.h5\n",
      "Epoch 7/35\n",
      "290/290 [==============================] - 31s 108ms/step - loss: 2.9915 - val_loss: 3.0369\n",
      "\n",
      "Epoch 00007: val_loss improved from 3.17320 to 3.03687, saving model to model.h5\n",
      "Epoch 8/35\n",
      "290/290 [==============================] - 31s 108ms/step - loss: 2.8271 - val_loss: 2.9060\n",
      "\n",
      "Epoch 00008: val_loss improved from 3.03687 to 2.90602, saving model to model.h5\n",
      "Epoch 9/35\n",
      "290/290 [==============================] - 31s 108ms/step - loss: 2.6704 - val_loss: 2.7757\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.90602 to 2.77570, saving model to model.h5\n",
      "Epoch 10/35\n",
      "290/290 [==============================] - 32s 112ms/step - loss: 2.5086 - val_loss: 2.6589\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.77570 to 2.65885, saving model to model.h5\n",
      "Epoch 11/35\n",
      "290/290 [==============================] - 33s 114ms/step - loss: 2.3604 - val_loss: 2.5558\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.65885 to 2.55577, saving model to model.h5\n",
      "Epoch 12/35\n",
      "290/290 [==============================] - 34s 116ms/step - loss: 2.2265 - val_loss: 2.4574\n",
      "\n",
      "Epoch 00012: val_loss improved from 2.55577 to 2.45742, saving model to model.h5\n",
      "Epoch 13/35\n",
      "290/290 [==============================] - 33s 113ms/step - loss: 2.0980 - val_loss: 2.3781\n",
      "\n",
      "Epoch 00013: val_loss improved from 2.45742 to 2.37812, saving model to model.h5\n",
      "Epoch 14/35\n",
      "290/290 [==============================] - 33s 113ms/step - loss: 1.9891 - val_loss: 2.3051\n",
      "\n",
      "Epoch 00014: val_loss improved from 2.37812 to 2.30509, saving model to model.h5\n",
      "Epoch 15/35\n",
      "290/290 [==============================] - 32s 109ms/step - loss: 1.8844 - val_loss: 2.2461\n",
      "\n",
      "Epoch 00015: val_loss improved from 2.30509 to 2.24613, saving model to model.h5\n",
      "Epoch 16/35\n",
      "290/290 [==============================] - 31s 108ms/step - loss: 1.7559 - val_loss: 2.1887\n",
      "\n",
      "Epoch 00016: val_loss improved from 2.24613 to 2.18868, saving model to model.h5\n",
      "Epoch 17/35\n",
      "290/290 [==============================] - 31s 107ms/step - loss: 1.6562 - val_loss: 2.1319\n",
      "\n",
      "Epoch 00017: val_loss improved from 2.18868 to 2.13185, saving model to model.h5\n",
      "Epoch 18/35\n",
      "290/290 [==============================] - 31s 107ms/step - loss: 1.5770 - val_loss: 2.0947\n",
      "\n",
      "Epoch 00018: val_loss improved from 2.13185 to 2.09470, saving model to model.h5\n",
      "Epoch 19/35\n",
      "290/290 [==============================] - 32s 110ms/step - loss: 1.4775 - val_loss: 2.0541\n",
      "\n",
      "Epoch 00019: val_loss improved from 2.09470 to 2.05414, saving model to model.h5\n",
      "Epoch 20/35\n",
      "290/290 [==============================] - 32s 109ms/step - loss: 1.3849 - val_loss: 2.0233\n",
      "\n",
      "Epoch 00020: val_loss improved from 2.05414 to 2.02334, saving model to model.h5\n",
      "Epoch 21/35\n",
      "290/290 [==============================] - 32s 110ms/step - loss: 1.3091 - val_loss: 1.9731\n",
      "\n",
      "Epoch 00021: val_loss improved from 2.02334 to 1.97310, saving model to model.h5\n",
      "Epoch 22/35\n",
      "290/290 [==============================] - 31s 108ms/step - loss: 1.2288 - val_loss: 1.9483\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.97310 to 1.94826, saving model to model.h5\n",
      "Epoch 23/35\n",
      "290/290 [==============================] - 32s 111ms/step - loss: 1.1464 - val_loss: 1.9239\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.94826 to 1.92387, saving model to model.h5\n",
      "Epoch 24/35\n",
      "290/290 [==============================] - 32s 112ms/step - loss: 1.0883 - val_loss: 1.8957\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.92387 to 1.89567, saving model to model.h5\n",
      "Epoch 25/35\n",
      "290/290 [==============================] - 31s 108ms/step - loss: 1.0253 - val_loss: 1.8741\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.89567 to 1.87414, saving model to model.h5\n",
      "Epoch 26/35\n",
      "290/290 [==============================] - 31s 108ms/step - loss: 0.9622 - val_loss: 1.8685\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.87414 to 1.86850, saving model to model.h5\n",
      "Epoch 27/35\n",
      "290/290 [==============================] - 31s 108ms/step - loss: 0.9196 - val_loss: 1.8395\n",
      "\n",
      "Epoch 00027: val_loss improved from 1.86850 to 1.83952, saving model to model.h5\n",
      "Epoch 28/35\n",
      "290/290 [==============================] - 31s 108ms/step - loss: 0.8387 - val_loss: 1.8217\n",
      "\n",
      "Epoch 00028: val_loss improved from 1.83952 to 1.82169, saving model to model.h5\n",
      "Epoch 29/35\n",
      "290/290 [==============================] - 31s 107ms/step - loss: 0.7856 - val_loss: 1.8088\n",
      "\n",
      "Epoch 00029: val_loss improved from 1.82169 to 1.80881, saving model to model.h5\n",
      "Epoch 30/35\n",
      "290/290 [==============================] - 31s 108ms/step - loss: 0.7352 - val_loss: 1.8064\n",
      "\n",
      "Epoch 00030: val_loss improved from 1.80881 to 1.80642, saving model to model.h5\n",
      "Epoch 31/35\n",
      "290/290 [==============================] - 31s 107ms/step - loss: 0.6934 - val_loss: 1.7976\n",
      "\n",
      "Epoch 00031: val_loss improved from 1.80642 to 1.79759, saving model to model.h5\n",
      "Epoch 32/35\n",
      "290/290 [==============================] - 31s 107ms/step - loss: 0.6521 - val_loss: 1.7798\n",
      "\n",
      "Epoch 00032: val_loss improved from 1.79759 to 1.77976, saving model to model.h5\n",
      "Epoch 33/35\n",
      "290/290 [==============================] - 31s 107ms/step - loss: 0.6122 - val_loss: 1.7828\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.77976\n",
      "Epoch 34/35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290/290 [==============================] - 32s 109ms/step - loss: 0.5803 - val_loss: 1.7815\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.77976\n",
      "Epoch 35/35\n",
      "290/290 [==============================] - 32s 109ms/step - loss: 0.5503 - val_loss: 1.7692\n",
      "\n",
      "Epoch 00035: val_loss improved from 1.77976 to 1.76920, saving model to model.h5\n",
      "train\n",
      "src=[vermissen sie boston], target=[do you miss boston], predicted=[do you like boston]\n",
      "src=[kann ich ihn nach hause mitnehmen], target=[can i take it home], predicted=[can i take home home]\n",
      "src=[fhlst du dich krank], target=[do you feel sick], predicted=[do you feeling]\n",
      "src=[sie spricht schnell], target=[she talks quickly], predicted=[she speaks quickly]\n",
      "src=[ich werde mit ihm schimpfen], target=[ill scold him], predicted=[ill scold him]\n",
      "src=[sie ist nicht arm], target=[she isnt poor], predicted=[she not poor]\n",
      "src=[lass uns jetzt gehen], target=[lets go now], predicted=[lets go now]\n",
      "src=[was ist so lustig], target=[whats so funny], predicted=[what so funny]\n",
      "src=[tom war eiferschtig], target=[tom was jealous], predicted=[tom was jealous]\n",
      "src=[mach das licht aus], target=[turn off the light], predicted=[turn off the light]\n",
      "BLEU-1: 0.826393\n",
      "BLEU-2: 0.752384\n",
      "BLEU-3: 0.693124\n",
      "BLEU-4: 0.515209\n",
      "test\n",
      "src=[das ist meine tasche], target=[this is my bag], predicted=[this is my bag]\n",
      "src=[hast du kinder], target=[do you have kids], predicted=[do you have kids]\n",
      "src=[seid ihr noch da], target=[are you still there], predicted=[are you still there]\n",
      "src=[da sind wir], target=[here we are], predicted=[here are are]\n",
      "src=[tom ist innovativ], target=[tom is innovative], predicted=[tom is gone]\n",
      "src=[sie ist aufgeschlossen], target=[shes openminded], predicted=[this is creepy]\n",
      "src=[ich versuche es noch mal], target=[ill try again], predicted=[i like it again]\n",
      "src=[wo ist meine uhr], target=[where is my watch], predicted=[wheres my my]\n",
      "src=[ich brauche einen job], target=[i need a job], predicted=[i need a money]\n",
      "src=[das wusste ich schon], target=[i already knew that], predicted=[i already that that]\n",
      "BLEU-1: 0.555234\n",
      "BLEU-2: 0.427169\n",
      "BLEU-3: 0.365530\n",
      "BLEU-4: 0.228521\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 10, 64)            399360    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "repeat_vector_3 (RepeatVecto (None, 6, 64)             0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 6, 64)             33024     \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 6, 3816)           248040    \n",
      "=================================================================\n",
      "Total params: 713,448\n",
      "Trainable params: 713,448\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n",
      "Epoch 1/35\n",
      "290/290 [==============================] - 25s 68ms/step - loss: 5.7147 - val_loss: 4.0055\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.00549, saving model to model.h5\n",
      "Epoch 2/35\n",
      "290/290 [==============================] - 18s 62ms/step - loss: 3.8878 - val_loss: 3.7130\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.00549 to 3.71296, saving model to model.h5\n",
      "Epoch 3/35\n",
      "290/290 [==============================] - 18s 62ms/step - loss: 3.6061 - val_loss: 3.5486\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.71296 to 3.54859, saving model to model.h5\n",
      "Epoch 4/35\n",
      "290/290 [==============================] - 18s 62ms/step - loss: 3.4470 - val_loss: 3.4644\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.54859 to 3.46441, saving model to model.h5\n",
      "Epoch 5/35\n",
      "290/290 [==============================] - 18s 62ms/step - loss: 3.3628 - val_loss: 3.4267\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.46441 to 3.42670, saving model to model.h5\n",
      "Epoch 6/35\n",
      "290/290 [==============================] - 18s 63ms/step - loss: 3.3208 - val_loss: 3.3954\n",
      "\n",
      "Epoch 00006: val_loss improved from 3.42670 to 3.39543, saving model to model.h5\n",
      "Epoch 7/35\n",
      "290/290 [==============================] - 18s 61ms/step - loss: 3.2656 - val_loss: 3.3499\n",
      "\n",
      "Epoch 00007: val_loss improved from 3.39543 to 3.34989, saving model to model.h5\n",
      "Epoch 8/35\n",
      "290/290 [==============================] - 18s 62ms/step - loss: 3.1995 - val_loss: 3.2861TA: 0s\n",
      "\n",
      "Epoch 00008: val_loss improved from 3.34989 to 3.28606, saving model to model.h5\n",
      "Epoch 9/35\n",
      "290/290 [==============================] - 18s 62ms/step - loss: 3.1325 - val_loss: 3.2262\n",
      "\n",
      "Epoch 00009: val_loss improved from 3.28606 to 3.22624, saving model to model.h5\n",
      "Epoch 10/35\n",
      "290/290 [==============================] - 18s 62ms/step - loss: 3.0466 - val_loss: 3.1541\n",
      "\n",
      "Epoch 00010: val_loss improved from 3.22624 to 3.15413, saving model to model.h5\n",
      "Epoch 11/35\n",
      "290/290 [==============================] - 18s 63ms/step - loss: 2.9653 - val_loss: 3.0823\n",
      "\n",
      "Epoch 00011: val_loss improved from 3.15413 to 3.08229, saving model to model.h5\n",
      "Epoch 12/35\n",
      "290/290 [==============================] - 18s 62ms/step - loss: 2.8798 - val_loss: 2.9953\n",
      "\n",
      "Epoch 00012: val_loss improved from 3.08229 to 2.99527, saving model to model.h5\n",
      "Epoch 13/35\n",
      "290/290 [==============================] - 18s 62ms/step - loss: 2.7532 - val_loss: 2.8996\n",
      "\n",
      "Epoch 00013: val_loss improved from 2.99527 to 2.89961, saving model to model.h5\n",
      "Epoch 14/35\n",
      "290/290 [==============================] - 18s 61ms/step - loss: 2.6470 - val_loss: 2.8177\n",
      "\n",
      "Epoch 00014: val_loss improved from 2.89961 to 2.81768, saving model to model.h5\n",
      "Epoch 15/35\n",
      "290/290 [==============================] - 18s 62ms/step - loss: 2.5380 - val_loss: 2.7476\n",
      "\n",
      "Epoch 00015: val_loss improved from 2.81768 to 2.74758, saving model to model.h5\n",
      "Epoch 16/35\n",
      "290/290 [==============================] - 18s 62ms/step - loss: 2.4498 - val_loss: 2.6897\n",
      "\n",
      "Epoch 00016: val_loss improved from 2.74758 to 2.68968, saving model to model.h5\n",
      "Epoch 17/35\n",
      "290/290 [==============================] - 18s 62ms/step - loss: 2.3707 - val_loss: 2.6345\n",
      "\n",
      "Epoch 00017: val_loss improved from 2.68968 to 2.63448, saving model to model.h5\n",
      "Epoch 18/35\n",
      "290/290 [==============================] - 18s 62ms/step - loss: 2.2813 - val_loss: 2.5849\n",
      "\n",
      "Epoch 00018: val_loss improved from 2.63448 to 2.58489, saving model to model.h5\n",
      "Epoch 19/35\n",
      "290/290 [==============================] - 18s 62ms/step - loss: 2.2200 - val_loss: 2.5401\n",
      "\n",
      "Epoch 00019: val_loss improved from 2.58489 to 2.54015, saving model to model.h5\n",
      "Epoch 20/35\n",
      "290/290 [==============================] - 18s 62ms/step - loss: 2.1489 - val_loss: 2.5016\n",
      "\n",
      "Epoch 00020: val_loss improved from 2.54015 to 2.50161, saving model to model.h5\n",
      "Epoch 21/35\n",
      "290/290 [==============================] - 18s 62ms/step - loss: 2.0761 - val_loss: 2.4733\n",
      "\n",
      "Epoch 00021: val_loss improved from 2.50161 to 2.47328, saving model to model.h5\n",
      "Epoch 22/35\n",
      "290/290 [==============================] - 18s 62ms/step - loss: 2.0226 - val_loss: 2.4398\n",
      "\n",
      "Epoch 00022: val_loss improved from 2.47328 to 2.43980, saving model to model.h5\n",
      "Epoch 23/35\n",
      "290/290 [==============================] - 18s 61ms/step - loss: 1.9707 - val_loss: 2.4146\n",
      "\n",
      "Epoch 00023: val_loss improved from 2.43980 to 2.41457, saving model to model.h5\n",
      "Epoch 24/35\n",
      "290/290 [==============================] - 18s 62ms/step - loss: 1.9199 - val_loss: 2.3799\n",
      "\n",
      "Epoch 00024: val_loss improved from 2.41457 to 2.37987, saving model to model.h5\n",
      "Epoch 25/35\n",
      "290/290 [==============================] - 18s 62ms/step - loss: 1.8664 - val_loss: 2.3571\n",
      "\n",
      "Epoch 00025: val_loss improved from 2.37987 to 2.35707, saving model to model.h5\n",
      "Epoch 26/35\n",
      "290/290 [==============================] - 18s 62ms/step - loss: 1.8218 - val_loss: 2.3328\n",
      "\n",
      "Epoch 00026: val_loss improved from 2.35707 to 2.33281, saving model to model.h5\n",
      "Epoch 27/35\n",
      "290/290 [==============================] - 18s 61ms/step - loss: 1.7576 - val_loss: 2.3100\n",
      "\n",
      "Epoch 00027: val_loss improved from 2.33281 to 2.30996, saving model to model.h5\n",
      "Epoch 28/35\n",
      "290/290 [==============================] - 17s 60ms/step - loss: 1.7050 - val_loss: 2.2936\n",
      "\n",
      "Epoch 00028: val_loss improved from 2.30996 to 2.29362, saving model to model.h5\n",
      "Epoch 29/35\n",
      "290/290 [==============================] - 18s 62ms/step - loss: 1.6730 - val_loss: 2.2687\n",
      "\n",
      "Epoch 00029: val_loss improved from 2.29362 to 2.26868, saving model to model.h5\n",
      "Epoch 30/35\n",
      "290/290 [==============================] - 18s 64ms/step - loss: 1.6192 - val_loss: 2.2465\n",
      "\n",
      "Epoch 00030: val_loss improved from 2.26868 to 2.24653, saving model to model.h5\n",
      "Epoch 31/35\n",
      "290/290 [==============================] - 18s 61ms/step - loss: 1.5777 - val_loss: 2.2377\n",
      "\n",
      "Epoch 00031: val_loss improved from 2.24653 to 2.23770, saving model to model.h5\n",
      "Epoch 32/35\n",
      "290/290 [==============================] - 18s 60ms/step - loss: 1.5235 - val_loss: 2.2196\n",
      "\n",
      "Epoch 00032: val_loss improved from 2.23770 to 2.21961, saving model to model.h5\n",
      "Epoch 33/35\n",
      "290/290 [==============================] - 18s 60ms/step - loss: 1.4925 - val_loss: 2.2097\n",
      "\n",
      "Epoch 00033: val_loss improved from 2.21961 to 2.20972, saving model to model.h5\n",
      "Epoch 34/35\n",
      "290/290 [==============================] - 17s 60ms/step - loss: 1.4512 - val_loss: 2.1913\n",
      "\n",
      "Epoch 00034: val_loss improved from 2.20972 to 2.19126, saving model to model.h5\n",
      "Epoch 35/35\n",
      "290/290 [==============================] - 18s 60ms/step - loss: 1.4021 - val_loss: 2.1856\n",
      "\n",
      "Epoch 00035: val_loss improved from 2.19126 to 2.18561, saving model to model.h5\n",
      "train\n",
      "src=[vermissen sie boston], target=[do you miss boston], predicted=[do you like money]\n",
      "src=[kann ich ihn nach hause mitnehmen], target=[can i take it home], predicted=[can i go to to]\n",
      "src=[fhlst du dich krank], target=[do you feel sick], predicted=[do you like]\n",
      "src=[sie spricht schnell], target=[she talks quickly], predicted=[she was]\n",
      "src=[ich werde mit ihm schimpfen], target=[ill scold him], predicted=[ill just him]\n",
      "src=[sie ist nicht arm], target=[she isnt poor], predicted=[she isnt not]\n",
      "src=[lass uns jetzt gehen], target=[lets go now], predicted=[lets go go]\n",
      "src=[was ist so lustig], target=[whats so funny], predicted=[how was fun]\n",
      "src=[tom war eiferschtig], target=[tom was jealous], predicted=[tom was young]\n",
      "src=[mach das licht aus], target=[turn off the light], predicted=[turn on the room]\n",
      "BLEU-1: 0.521945\n",
      "BLEU-2: 0.369077\n",
      "BLEU-3: 0.280288\n",
      "BLEU-4: 0.136050\n",
      "test\n",
      "src=[das ist meine tasche], target=[this is my bag], predicted=[this is is bag]\n",
      "src=[hast du kinder], target=[do you have kids], predicted=[do you have money]\n",
      "src=[seid ihr noch da], target=[are you still there], predicted=[are you home here]\n",
      "src=[da sind wir], target=[here we are], predicted=[were all here]\n",
      "src=[tom ist innovativ], target=[tom is innovative], predicted=[tom is]\n",
      "src=[sie ist aufgeschlossen], target=[shes openminded], predicted=[its is]\n",
      "src=[ich versuche es noch mal], target=[ill try again], predicted=[i just to it]\n",
      "src=[wo ist meine uhr], target=[where is my watch], predicted=[wheres is my]\n",
      "src=[ich brauche einen job], target=[i need a job], predicted=[i need a lot]\n",
      "src=[das wusste ich schon], target=[i already knew that], predicted=[i saw it it]\n",
      "BLEU-1: 0.412500\n",
      "BLEU-2: 0.265715\n",
      "BLEU-3: 0.199022\n",
      "BLEU-4: 0.090734\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 10, 512)           3194880   \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "repeat_vector_4 (RepeatVecto (None, 6, 512)            0         \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 6, 512)            2099200   \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 6, 3816)           1957608   \n",
      "=================================================================\n",
      "Total params: 9,350,888\n",
      "Trainable params: 9,350,888\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n",
      "Epoch 1/35\n",
      "290/290 [==============================] - 110s 353ms/step - loss: 4.4538 - val_loss: 3.3816\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.38155, saving model to model.h5\n",
      "Epoch 2/35\n",
      "290/290 [==============================] - 97s 336ms/step - loss: 3.2632 - val_loss: 3.0575\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.38155 to 3.05753, saving model to model.h5\n",
      "Epoch 3/35\n",
      "290/290 [==============================] - 99s 342ms/step - loss: 2.8501 - val_loss: 2.6628\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.05753 to 2.66283, saving model to model.h5\n",
      "Epoch 4/35\n",
      "290/290 [==============================] - 106s 367ms/step - loss: 2.4184 - val_loss: 2.3629\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.66283 to 2.36293, saving model to model.h5\n",
      "Epoch 5/35\n",
      "290/290 [==============================] - 100s 346ms/step - loss: 2.0425 - val_loss: 2.1578\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.36293 to 2.15783, saving model to model.h5\n",
      "Epoch 6/35\n",
      "290/290 [==============================] - 101s 348ms/step - loss: 1.7278 - val_loss: 2.0057\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.15783 to 2.00574, saving model to model.h5\n",
      "Epoch 7/35\n",
      "290/290 [==============================] - 107s 368ms/step - loss: 1.4644 - val_loss: 1.8853\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.00574 to 1.88533, saving model to model.h5\n",
      "Epoch 8/35\n",
      "290/290 [==============================] - 104s 357ms/step - loss: 1.2280 - val_loss: 1.7821\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.88533 to 1.78206, saving model to model.h5\n",
      "Epoch 9/35\n",
      "290/290 [==============================] - 106s 365ms/step - loss: 1.0119 - val_loss: 1.7136\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.78206 to 1.71360, saving model to model.h5\n",
      "Epoch 10/35\n",
      "290/290 [==============================] - 134s 461ms/step - loss: 0.8357 - val_loss: 1.6527\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.71360 to 1.65274, saving model to model.h5\n",
      "Epoch 11/35\n",
      "290/290 [==============================] - 134s 463ms/step - loss: 0.6835 - val_loss: 1.6290\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.65274 to 1.62904, saving model to model.h5\n",
      "Epoch 12/35\n",
      "290/290 [==============================] - 133s 460ms/step - loss: 0.5708 - val_loss: 1.5927\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.62904 to 1.59266, saving model to model.h5\n",
      "Epoch 13/35\n",
      "290/290 [==============================] - 133s 459ms/step - loss: 0.4550 - val_loss: 1.5866\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.59266 to 1.58662, saving model to model.h5\n",
      "Epoch 14/35\n",
      "290/290 [==============================] - 135s 464ms/step - loss: 0.3931 - val_loss: 1.5791\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.58662 to 1.57912, saving model to model.h5\n",
      "Epoch 15/35\n",
      "290/290 [==============================] - 133s 459ms/step - loss: 0.3192 - val_loss: 1.5882\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.57912\n",
      "Epoch 16/35\n",
      "290/290 [==============================] - 134s 461ms/step - loss: 0.2704 - val_loss: 1.5949\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.57912\n",
      "Epoch 17/35\n",
      "290/290 [==============================] - 134s 462ms/step - loss: 0.2275 - val_loss: 1.5844\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.57912\n",
      "Epoch 18/35\n",
      "290/290 [==============================] - 135s 464ms/step - loss: 0.1926 - val_loss: 1.6130\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.57912\n",
      "Epoch 19/35\n",
      "290/290 [==============================] - 106s 366ms/step - loss: 0.1650 - val_loss: 1.6181\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.57912\n",
      "Epoch 20/35\n",
      "290/290 [==============================] - 94s 324ms/step - loss: 0.1518 - val_loss: 1.6421\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.57912\n",
      "Epoch 21/35\n",
      "290/290 [==============================] - 96s 332ms/step - loss: 0.1457 - val_loss: 1.6515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00021: val_loss did not improve from 1.57912\n",
      "Epoch 22/35\n",
      "290/290 [==============================] - 96s 332ms/step - loss: 0.1489 - val_loss: 1.6785\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.57912\n",
      "Epoch 23/35\n",
      "290/290 [==============================] - 96s 332ms/step - loss: 0.1278 - val_loss: 1.6741\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.57912\n",
      "Epoch 24/35\n",
      "290/290 [==============================] - 96s 333ms/step - loss: 0.1190 - val_loss: 1.6847\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.57912\n",
      "Epoch 25/35\n",
      "290/290 [==============================] - 98s 339ms/step - loss: 0.1151 - val_loss: 1.7204\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.57912\n",
      "Epoch 26/35\n",
      "290/290 [==============================] - 97s 333ms/step - loss: 0.1106 - val_loss: 1.7306\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.57912\n",
      "Epoch 27/35\n",
      "290/290 [==============================] - 97s 334ms/step - loss: 0.1127 - val_loss: 1.7168\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.57912\n",
      "Epoch 28/35\n",
      "290/290 [==============================] - 96s 332ms/step - loss: 0.1098 - val_loss: 1.7383\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.57912\n",
      "Epoch 29/35\n",
      "290/290 [==============================] - 96s 332ms/step - loss: 0.1046 - val_loss: 1.7452\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.57912\n",
      "Epoch 30/35\n",
      "290/290 [==============================] - 97s 334ms/step - loss: 0.1072 - val_loss: 1.7699\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.57912\n",
      "Epoch 31/35\n",
      "290/290 [==============================] - 97s 334ms/step - loss: 0.1028 - val_loss: 1.7512\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.57912\n",
      "Epoch 32/35\n",
      "290/290 [==============================] - 97s 335ms/step - loss: 0.1027 - val_loss: 1.7740\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.57912\n",
      "Epoch 33/35\n",
      "290/290 [==============================] - 97s 334ms/step - loss: 0.0992 - val_loss: 1.7798\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.57912\n",
      "Epoch 34/35\n",
      "290/290 [==============================] - 97s 335ms/step - loss: 0.1004 - val_loss: 1.8055\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.57912\n",
      "Epoch 35/35\n",
      "290/290 [==============================] - 97s 334ms/step - loss: 0.0970 - val_loss: 1.8060\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.57912\n",
      "train\n",
      "src=[vermissen sie boston], target=[do you miss boston], predicted=[do you miss boston]\n",
      "src=[kann ich ihn nach hause mitnehmen], target=[can i take it home], predicted=[can i take it home]\n",
      "src=[fhlst du dich krank], target=[do you feel sick], predicted=[do you feel sick]\n",
      "src=[sie spricht schnell], target=[she talks quickly], predicted=[she talks quickly]\n",
      "src=[ich werde mit ihm schimpfen], target=[ill scold him], predicted=[ill scold him]\n",
      "src=[sie ist nicht arm], target=[she isnt poor], predicted=[she isnt poor]\n",
      "src=[lass uns jetzt gehen], target=[lets go now], predicted=[lets go go]\n",
      "src=[was ist so lustig], target=[whats so funny], predicted=[what is]\n",
      "src=[tom war eiferschtig], target=[tom was jealous], predicted=[tom was jealous]\n",
      "src=[mach das licht aus], target=[turn off the light], predicted=[turn off the light]\n",
      "BLEU-1: 0.894312\n",
      "BLEU-2: 0.850275\n",
      "BLEU-3: 0.807794\n",
      "BLEU-4: 0.647360\n",
      "test\n",
      "src=[das ist meine tasche], target=[this is my bag], predicted=[this bag is bag]\n",
      "src=[hast du kinder], target=[do you have kids], predicted=[do you have kids]\n",
      "src=[seid ihr noch da], target=[are you still there], predicted=[are you still there]\n",
      "src=[da sind wir], target=[here we are], predicted=[here we are]\n",
      "src=[tom ist innovativ], target=[tom is innovative], predicted=[tom is unmerciful]\n",
      "src=[sie ist aufgeschlossen], target=[shes openminded], predicted=[she is picky]\n",
      "src=[ich versuche es noch mal], target=[ill try again], predicted=[i almost it it]\n",
      "src=[wo ist meine uhr], target=[where is my watch], predicted=[where is my clock]\n",
      "src=[ich brauche einen job], target=[i need a job], predicted=[i need a job]\n",
      "src=[das wusste ich schon], target=[i already knew that], predicted=[i already knew this]\n",
      "BLEU-1: 0.598876\n",
      "BLEU-2: 0.486882\n",
      "BLEU-3: 0.433481\n",
      "BLEU-4: 0.289371\n"
     ]
    }
   ],
   "source": [
    "# n_units in embedding layer\n",
    "for i in [256, 128, 64, 512]:\n",
    "    # define model\n",
    "    model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, i)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "    # summarize defined model\n",
    "    print(model.summary())\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    # fit model\n",
    "    filename = 'model.h5'\n",
    "    checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    model.fit(trainX, trainY, epochs=35, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=1)\n",
    "\n",
    "    # load model\n",
    "    model = load_model('model.h5')\n",
    "    # test on some training sequences\n",
    "    print('train')\n",
    "    evaluate_model(model, eng_tokenizer, trainX, train)\n",
    "    # test on some test sequences\n",
    "    print('test')\n",
    "    evaluate_model(model, eng_tokenizer, testX, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
